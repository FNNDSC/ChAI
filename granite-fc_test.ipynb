{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d6434e-b8a0-4654-b11c-c0da9ef3bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "\n",
    "!pip install fastapi uvicorn nest_asyncio --quiet\n",
    "!pip install langchain==0.0.208 --quiet\n",
    "!pip install pydantic==1.10.12 --quiet\n",
    "!pip install requests --quiet\n",
    "!pip install openai --quiet  # Required by LangChain agents\n",
    "!pip install tenacity --quiet  # Required by OpenAI package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa9a231-d8b9-4a16-891a-1ff4b84da29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1471]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import threading\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import requests\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Replace with your OpenWeatherMap API key\n",
    "API_KEY = \"2e198987f31b1681043f45fcbed481c4\"  # <-- Replace with your actual OpenWeatherMap API key\n",
    "\n",
    "class WeatherRequest(BaseModel):\n",
    "    location: str\n",
    "    date: Optional[str] = None\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str\n",
    "    date: str\n",
    "    forecast: str\n",
    "\n",
    "# Define a function to fetch weather data from OpenWeatherMap\n",
    "def fetch_weather_from_api(location: str) -> str:\n",
    "    try:\n",
    "        # OpenWeatherMap API endpoint\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={API_KEY}&units=metric\"\n",
    "        \n",
    "        response = requests.get(url, timeout=10)  # Timeout after 10 seconds\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Extract necessary weather information\n",
    "            weather_description = data['weather'][0]['description']\n",
    "            temperature = data['main']['temp']\n",
    "            forecast = f\"The weather in {location} is {weather_description} with a temperature of {temperature}Â°C.\"\n",
    "            return forecast\n",
    "        else:\n",
    "            raise HTTPException(status_code=response.status_code, detail=f\"API Error {response.status_code}: {response.text}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise HTTPException(status_code=504, detail=\"The request to get the weather data timed out.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"An error occurred while fetching the weather data: {e}\")\n",
    "\n",
    "# FastAPI endpoint to get weather data\n",
    "@app.post(\"/get_weather\", response_model=WeatherResponse)\n",
    "def get_weather(request: WeatherRequest):\n",
    "    # Fetch weather data from OpenWeatherMap API\n",
    "    forecast = fetch_weather_from_api(request.location)\n",
    "    \n",
    "    return WeatherResponse(\n",
    "        location=request.location,\n",
    "        date=request.date or \"today\",\n",
    "        forecast=forecast\n",
    "    )\n",
    "\n",
    "# Run the FastAPI app in a separate thread\n",
    "def run_app():\n",
    "    uvicorn.run(app, host='127.0.0.1', port=8000)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "app_thread = threading.Thread(target=run_app, daemon=True)\n",
    "app_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a042cf1-563f-40ad-a596-7c3a66ee8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create a Custom LLM Class for the API\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "# Suppress InsecureRequestWarning if using verify=False\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "class GraniteAPI(LLM):\n",
    "    api_key: str\n",
    "    model_name: str = \"granite-8b-code-instruct-128k\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GraniteAPI\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"{self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9,\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/completions\",\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=30,       # Timeout after 30 seconds\n",
    "                verify=False      # Disable SSL verification temporarily\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "            else:\n",
    "                raise Exception(f\"API Error {response.status_code}: {response.text}\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise Exception(\"Request timed out. The server did not respond within 30 seconds.\")\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            raise Exception(f\"SSL Error: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Request Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74d3058-26dc-4705-9daa-5b64a2a374ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate the LLM Using LangChain\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "# Replace 'your_api_key' with your actual API key\n",
    "api_key = \"2d7706908f748b753f9052a233952657\"\n",
    "\n",
    "# Initialize the custom LLM\n",
    "llm = GraniteAPI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bdff0c-09e4-4a93-b07c-dfb30fdea5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Implement the Function Calling Logic Using LangChain with Enhanced Prompt\n",
    "\n",
    "import requests\n",
    "from langchain.agents import Tool, ZeroShotAgent, AgentExecutor\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate  # Import PromptTemplate\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "# Define the tool (function) that the agent can use\n",
    "def get_weather_tool(location: str, date: Optional[str] = None) -> str:\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:8000/get_weather\",\n",
    "            json={\"location\": location, \"date\": date},\n",
    "            timeout=10  # Timeout after 10 seconds\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data['forecast']\n",
    "        else:\n",
    "            raise Exception(f\"API Error {response.status_code}: {response.text}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"The request to get the weather data timed out.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred while fetching the weather data: {e}\"\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=get_weather_tool,\n",
    "    description=\"Use this tool to get the weather forecast for a specific location and date.\"\n",
    ")\n",
    "\n",
    "# Define the custom prompt with explicit instructions\n",
    "custom_prompt_template = \"\"\"You are an intelligent assistant that helps answer questions by thinking step-by-step and performing actions when necessary.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tool_descriptions}\n",
    "\n",
    "When answering, use the following format exactly:\n",
    "\n",
    "Thought: [Your reasoning]\n",
    "Action: [The action to take, should be one of [{tool_names}]]\n",
    "Action Input: [The input to the action]\n",
    "Observation: [The result of the action]\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: [Your final reasoning]\n",
    "Final Answer: [The final answer to the user's question.]\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "# Create a custom PromptTemplate\n",
    "tool_descriptions = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in [weather_tool]])\n",
    "tool_names = \", \".join([tool.name for tool in [weather_tool]])\n",
    "prompt = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\n",
    "        \"tool_descriptions\": tool_descriptions,\n",
    "        \"tool_names\": tool_names\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a new LLMChain with the custom PromptTemplate\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define the custom output parser\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"\n",
    "        Parses the LLM output to extract the final answer.\n",
    "        If the expected ReAct format is not strictly followed, it tries to extract the Final Answer.\n",
    "        \"\"\"\n",
    "        if \"Final Answer:\" in text:\n",
    "            # Split the text by 'Final Answer:' and return the part after it\n",
    "            final_answer = text.split(\"Final Answer:\")[-1].strip()\n",
    "            return AgentFinish({\"output\": final_answer}, final_answer)\n",
    "        else:\n",
    "            # If 'Final Answer:' is not found, return the entire text as the output\n",
    "            return AgentFinish({\"output\": text.strip()}, text.strip())\n",
    "\n",
    "# Initialize the ZeroShotAgent with the custom LLMChain and output parser\n",
    "agent = ZeroShotAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    tools=[weather_tool],\n",
    "    output_parser=CustomOutputParser()\n",
    ")\n",
    "\n",
    "# Create an AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[weather_tool],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaedf9ae-07ab-43ad-8104-b6c1be2827cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe weather in Boston tomorrow is sunny with a high of 70 degrees and a low of 50 degrees. I recommend visiting the Freedom Trail, the Boston Public Garden, and the Boston Marathon.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Response to the User:\n",
      "The weather in Boston tomorrow is sunny with a high of 70 degrees and a low of 50 degrees. I recommend visiting the Freedom Trail, the Boston Public Garden, and the Boston Marathon.\n"
     ]
    }
   ],
   "source": [
    "# Corrected test code\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"What's the exact weather in Boston tomorrow? Based on the weather, can you recommend places I can visit in Boston?\"\n",
    "\n",
    "# Prepare the input dictionary\n",
    "input_data = {\n",
    "    \"input\": user_input,\n",
    "    \"agent_scratchpad\": \"\"  # Empty scratchpad for single-turn interaction\n",
    "}\n",
    "\n",
    "# Run the agent with the input dictionary using the agent_executor\n",
    "try:\n",
    "    response = agent_executor.run(input_data)\n",
    "    print(\"\\nFinal Response to the User:\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83317ad6-6494-4e42-93a8-5a79fda1747f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
